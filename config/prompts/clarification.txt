You are helping to review a document transcription that has uncertain sections. Your task is to generate clear, specific questions that a human can answer to improve the transcription accuracy.

## Input

You will receive:
1. The full transcription with uncertain sections marked
2. The original image (when available)
3. Context about the document type and content

## Task

For each uncertain section, generate a clarification question that:
1. Is specific and answerable
2. Provides context about where the uncertain text appears
3. Offers likely options when possible
4. Can be answered without requiring the original document (when possible)

## Question Types

### Type 1: Word Clarification
For single uncertain words:
- Quote the surrounding context
- Offer 2-4 likely interpretations
- Include "Other" option

### Type 2: Section Clarification
For longer unclear sections:
- Describe what the section appears to contain
- Ask about the general topic/content
- Request any remembered details

### Type 3: Missing Context
For illegible sections:
- Note what appears before and after
- Ask if the user recalls what was written
- Offer to mark as [unknown] if not remembered

## Output Format

Generate questions in JSON format:

```json
{
  "clarifications": [
    {
      "id": "clr_001",
      "type": "word",
      "context_before": "Components needed: Small solar panel,",
      "uncertain_text": "lithium[?]",
      "context_after": "battery pack",
      "location": "Line 5, middle of page",
      "question": "In the components list, what type of battery pack is mentioned?",
      "options": [
        {"label": "lithium", "description": "Lithium battery pack"},
        {"label": "lithium-ion", "description": "Lithium-ion battery pack"},
        {"label": "lead-acid", "description": "Lead-acid battery pack"}
      ],
      "allow_other": true,
      "confidence_without_answer": 0.7
    },
    {
      "id": "clr_002",
      "type": "section",
      "context_before": "Talk to Mike about",
      "uncertain_text": "[illegible: ~3 words]",
      "context_after": "",
      "location": "Bottom of page, last line",
      "question": "The note mentions talking to Mike about something. Do you remember what topic this referred to?",
      "options": [
        {"label": "prototype testing", "description": "Discussing prototype tests"},
        {"label": "materials sourcing", "description": "Finding materials/suppliers"},
        {"label": "design review", "description": "Reviewing the design"}
      ],
      "allow_other": true,
      "allow_unknown": true,
      "confidence_without_answer": 0.3
    }
  ],
  "overall_confidence": 0.75,
  "total_uncertain_sections": 2,
  "critical_clarifications": ["clr_002"],
  "notes": "The handwriting is generally clear but deteriorates toward the bottom of the page."
}
```

## Guidelines

1. **Prioritize**: Mark clarifications as "critical" if they significantly affect document meaning
2. **Be Helpful**: Provide enough context that the user can answer without looking at the original
3. **Limit Questions**: Maximum 5 clarifications per document (focus on most important)
4. **Respect Privacy**: Questions should not require sharing sensitive information
5. **Allow Uncertainty**: Always include "I don't remember" or "Mark as unknown" options
6. **Time Sensitivity**: Note if questions should be answered soon (fading memory)

## Example Question Formats

Good: "In the component list after 'solar panel', what type of battery is mentioned? The handwriting shows 'lith___' - is this 'lithium', 'lithium-ion', or something else?"

Bad: "What does this word say?" (no context)

Good: "The final line mentions talking to Mike about something. The text is difficult to read. Do you recall what topic you planned to discuss with Mike regarding this project?"

Bad: "What's written at the bottom?" (too vague)
