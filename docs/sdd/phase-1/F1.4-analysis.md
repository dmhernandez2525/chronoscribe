# F1.4 - AI Analysis & Metadata Extraction

## Overview

| Attribute | Value |
|-----------|-------|
| Feature ID | F1.4 |
| Phase | 1 (MVP) |
| Priority | P0 |
| Status | Planned |
| Module | `chronoscribe.services.analysis` |

## Description

Use LLMs (Claude or GPT-4o) to analyze transcribed documents and extract structured metadata including document type, dates, summary, entities, and suggested tags. This provides the intelligence layer that makes documents searchable and organizable.

## Requirements

### Functional Requirements

1. **FR1.4.1** - Classify document into predefined types
2. **FR1.4.2** - Extract dates with confidence scores
3. **FR1.4.3** - Generate 2-3 sentence summary
4. **FR1.4.4** - Extract named entities (people, places, concepts)
5. **FR1.4.5** - Suggest relevant tags
6. **FR1.4.6** - Assess transcription quality
7. **FR1.4.7** - Detect document language

### Non-Functional Requirements

1. **NFR1.4.1** - Analysis time: < 10 seconds per document
2. **NFR1.4.2** - Classification accuracy: > 90%
3. **NFR1.4.3** - Cost per document: < $0.02

## Technical Design

### Dependencies

```python
anthropic>=0.8.0    # Claude API
openai>=1.6.0       # GPT-4o API (fallback)
pydantic>=2.5.0     # Response validation
```

### Module Structure

```
src/chronoscribe/services/
├── analysis.py         # Main analysis service
└── prompts/
    └── analysis.py     # Prompt templates
```

### Type Definitions

```python
from dataclasses import dataclass, field
from datetime import date
from enum import Enum
from typing import Literal

class DocumentType(Enum):
    SKETCH = "sketch"
    NOTES = "notes"
    TYPED = "typed"
    MIXED = "mixed"
    LETTER = "letter"
    FORM = "form"
    JOURNAL = "journal"
    INVENTION = "invention"
    RECIPE = "recipe"
    FINANCIAL = "financial"
    MEDICAL = "medical"
    LEGAL = "legal"
    OTHER = "other"

@dataclass
class ExtractedDate:
    """A date extracted from the document."""
    value: date
    source: Literal["explicit", "exif", "filename", "inferred"]
    context: str
    confidence: float

@dataclass
class EntityCollection:
    """Named entities extracted from document."""
    people: list[str] = field(default_factory=list)
    organizations: list[str] = field(default_factory=list)
    locations: list[str] = field(default_factory=list)
    products: list[str] = field(default_factory=list)
    concepts: list[str] = field(default_factory=list)

@dataclass
class QualityAssessment:
    """Assessment of transcription quality."""
    text_clarity: int  # 1-10
    completeness: int  # 1-10
    overall_score: int  # 1-10
    notes: str | None = None

@dataclass
class AnalysisResult:
    """Complete analysis result."""
    document_type: DocumentType
    document_type_confidence: float
    dates: list[ExtractedDate]
    primary_date: date | None
    primary_date_confidence: float | None
    summary: str
    entities: EntityCollection
    suggested_tags: list[str]
    quality: QualityAssessment
    language: str
    processing_time_ms: int
```

### Analysis Service

```python
from anthropic import Anthropic
from openai import OpenAI
import json

class DocumentAnalyzer:
    """
    Analyzes transcribed documents using LLMs.

    Uses Claude by default, with GPT-4o as fallback.
    """

    def __init__(
        self,
        anthropic_client: Anthropic | None = None,
        openai_client: OpenAI | None = None,
        primary_provider: Literal["anthropic", "openai"] = "anthropic",
    ):
        self.anthropic = anthropic_client
        self.openai = openai_client
        self.primary_provider = primary_provider

    async def analyze(
        self,
        transcription: str,
        image_path: Path | None = None,
        metadata: dict | None = None,
    ) -> AnalysisResult:
        """
        Analyze a document transcription.

        Args:
            transcription: The transcribed text
            image_path: Optional path to image for visual context
            metadata: Optional existing metadata (EXIF, filename, etc.)

        Returns:
            AnalysisResult with extracted information
        """
        prompt = self._build_prompt(transcription, metadata)

        try:
            if self.primary_provider == "anthropic" and self.anthropic:
                response = await self._call_anthropic(prompt)
            elif self.openai:
                response = await self._call_openai(prompt)
            else:
                raise AnalysisError("No analysis provider configured")

            return self._parse_response(response)

        except Exception as e:
            # Try fallback provider
            if self.primary_provider == "anthropic" and self.openai:
                response = await self._call_openai(prompt)
                return self._parse_response(response)
            raise AnalysisError(str(e)) from e

    async def _call_anthropic(self, prompt: str) -> dict:
        """Call Claude API."""
        message = self.anthropic.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
        )

        # Parse JSON from response
        return json.loads(message.content[0].text)

    async def _call_openai(self, prompt: str) -> dict:
        """Call GPT-4o API."""
        response = self.openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            response_format={"type": "json_object"},
        )

        return json.loads(response.choices[0].message.content)

    def _build_prompt(
        self,
        transcription: str,
        metadata: dict | None,
    ) -> str:
        """Build the analysis prompt."""
        # Load prompt template from config/prompts/analysis.txt
        template = self._load_prompt_template()

        context = ""
        if metadata:
            if "exif_date" in metadata:
                context += f"EXIF Date: {metadata['exif_date']}\n"
            if "filename" in metadata:
                context += f"Filename: {metadata['filename']}\n"

        return template.format(
            transcription=transcription,
            context=context,
        )

    def _parse_response(self, response: dict) -> AnalysisResult:
        """Parse LLM response into AnalysisResult."""
        # Map string to enum
        doc_type = DocumentType(response.get("document_type", "other"))

        # Parse dates
        dates = []
        for date_info in response.get("dates", []):
            try:
                dates.append(ExtractedDate(
                    value=date.fromisoformat(date_info["value"]),
                    source=date_info["source"],
                    context=date_info["context"],
                    confidence=date_info["confidence"],
                ))
            except (ValueError, KeyError):
                continue

        # Primary date
        primary_date = None
        primary_date_confidence = None
        if response.get("primary_date"):
            try:
                primary_date = date.fromisoformat(response["primary_date"])
                primary_date_confidence = response.get("primary_date_confidence")
            except ValueError:
                pass

        # Entities
        entities_data = response.get("entities", {})
        entities = EntityCollection(
            people=entities_data.get("people", []),
            organizations=entities_data.get("organizations", []),
            locations=entities_data.get("locations", []),
            products=entities_data.get("products", []),
            concepts=entities_data.get("concepts", []),
        )

        # Quality
        quality_data = response.get("quality", {})
        quality = QualityAssessment(
            text_clarity=quality_data.get("text_clarity", 5),
            completeness=quality_data.get("completeness", 5),
            overall_score=quality_data.get("overall_score", 5),
            notes=quality_data.get("notes"),
        )

        return AnalysisResult(
            document_type=doc_type,
            document_type_confidence=response.get("document_type_confidence", 0.5),
            dates=dates,
            primary_date=primary_date,
            primary_date_confidence=primary_date_confidence,
            summary=response.get("summary", ""),
            entities=entities,
            suggested_tags=response.get("suggested_tags", []),
            quality=quality,
            language=response.get("language", "en"),
            processing_time_ms=0,  # Set by caller
        )
```

### Prompt Template

Located at `config/prompts/analysis.txt`:

```
You are an expert document analyst. Analyze the provided document transcription and extract structured metadata.

## Document Transcription

{transcription}

## Additional Context

{context}

## Tasks

1. Classify the document type
2. Extract any dates mentioned
3. Write a 2-3 sentence summary
4. Extract named entities
5. Suggest 3-5 tags
6. Assess quality

## Response Format

Respond with valid JSON:

{
  "document_type": "invention",
  "document_type_confidence": 0.9,
  "dates": [...],
  "primary_date": "2019-03-15",
  "primary_date_confidence": 0.95,
  "summary": "...",
  "entities": {...},
  "suggested_tags": [...],
  "quality": {...},
  "language": "en"
}
```

## API

### Public Interface

```python
async def analyze_document(
    transcription: str,
    image_path: Path | None = None,
    metadata: dict | None = None,
) -> AnalysisResult:
    """
    Analyze a document transcription to extract metadata.

    Args:
        transcription: The transcribed text
        image_path: Optional path to image for visual context
        metadata: Optional existing metadata

    Returns:
        AnalysisResult with document type, dates, summary, etc.

    Raises:
        AnalysisError: If analysis fails
    """
```

## Testing Strategy

### Unit Tests

```python
def test_parse_response_handles_valid_json():
    analyzer = DocumentAnalyzer()
    response = {
        "document_type": "invention",
        "document_type_confidence": 0.9,
        "dates": [{"value": "2019-03-15", "source": "explicit", "context": "Header", "confidence": 0.95}],
        "primary_date": "2019-03-15",
        "summary": "Test summary",
        "entities": {"people": ["Mike"]},
        "suggested_tags": ["invention"],
        "quality": {"overall_score": 8},
    }

    result = analyzer._parse_response(response)

    assert result.document_type == DocumentType.INVENTION
    assert result.primary_date == date(2019, 3, 15)
    assert "Mike" in result.entities.people

def test_parse_response_handles_missing_fields():
    analyzer = DocumentAnalyzer()
    response = {"document_type": "notes"}

    result = analyzer._parse_response(response)

    assert result.document_type == DocumentType.NOTES
    assert result.primary_date is None
    assert result.suggested_tags == []
```

## Dependencies

### Upstream
- Transcription service (provides text)
- Configuration (for provider settings)

### Downstream
- Timeline service (receives dates)
- Categorization service (receives document type)
- Database (stores analysis)
- Clarification service (receives quality assessment)

## Acceptance Criteria

- [ ] Document type classification works correctly
- [ ] Date extraction handles multiple formats
- [ ] Summary is concise and accurate
- [ ] Entity extraction identifies people, places, concepts
- [ ] Tag suggestions are relevant
- [ ] Quality assessment reflects actual quality
- [ ] Language detection works
- [ ] Fallback to secondary provider works
- [ ] Analysis completes in < 10 seconds
