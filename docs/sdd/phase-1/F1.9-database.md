# F1.9 - SQLite Database & FTS Search

## Overview

| Attribute | Value |
|-----------|-------|
| Feature ID | F1.9 |
| Phase | 1 (MVP) |
| Priority | P0 |
| Status | Planned |
| Module | `chronoscribe.storage.database` |

## Description

SQLite database with FTS5 (Full-Text Search) for storing document metadata, managing relationships, and enabling fast search across all transcribed content.

## Requirements

### Functional Requirements

1. **FR1.9.1** - Store document metadata (dates, types, paths, etc.)
2. **FR1.9.2** - Full-text search across transcriptions
3. **FR1.9.3** - Category relationships (many-to-many)
4. **FR1.9.4** - Entity storage
5. **FR1.9.5** - Clarification queue persistence
6. **FR1.9.6** - API usage tracking
7. **FR1.9.7** - Processing queue management

### Non-Functional Requirements

1. **NFR1.9.1** - Search response: < 500ms
2. **NFR1.9.2** - WAL mode for concurrent access
3. **NFR1.9.3** - Automatic backups

## Technical Design

### Dependencies

```python
aiosqlite>=0.19.0    # Async SQLite
```

### Schema

```sql
-- Enable WAL mode for better concurrency
PRAGMA journal_mode=WAL;
PRAGMA foreign_keys=ON;

-- Core document table
CREATE TABLE documents (
    id TEXT PRIMARY KEY,
    original_filename TEXT NOT NULL,
    archive_path TEXT NOT NULL,
    processed_at DATETIME NOT NULL,
    document_type TEXT NOT NULL,
    date_value DATE,
    date_confidence REAL,
    date_source TEXT,
    summary TEXT,
    word_count INTEGER,
    quality_score REAL,
    language TEXT DEFAULT 'en',
    provider_used TEXT,
    processing_time_ms INTEGER,
    file_size_original INTEGER,
    file_size_normalized INTEGER,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_documents_date ON documents(date_value);
CREATE INDEX idx_documents_type ON documents(document_type);
CREATE INDEX idx_documents_processed ON documents(processed_at);

-- Full-text search index
CREATE VIRTUAL TABLE documents_fts USING fts5(
    id,
    transcription,
    summary,
    entities_text,
    tokenize='porter unicode61'
);

-- Triggers to keep FTS in sync
CREATE TRIGGER documents_ai AFTER INSERT ON documents BEGIN
    INSERT INTO documents_fts(id, transcription, summary, entities_text)
    VALUES (new.id, '', new.summary, '');
END;

CREATE TRIGGER documents_ad AFTER DELETE ON documents BEGIN
    DELETE FROM documents_fts WHERE id = old.id;
END;

-- Categories
CREATE TABLE categories (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    description TEXT,
    parent_id TEXT REFERENCES categories(id),
    is_custom INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE document_categories (
    document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
    category_id TEXT REFERENCES categories(id) ON DELETE CASCADE,
    confidence REAL NOT NULL,
    is_primary INTEGER DEFAULT 0,
    is_manual INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (document_id, category_id)
);

CREATE INDEX idx_doc_cat_category ON document_categories(category_id);

-- Entities
CREATE TABLE entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
    entity_type TEXT NOT NULL,
    entity_value TEXT NOT NULL,
    confidence REAL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_entities_document ON entities(document_id);
CREATE INDEX idx_entities_type ON entities(entity_type);

-- Clarification queue
CREATE TABLE clarifications (
    id TEXT PRIMARY KEY,
    document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
    type TEXT NOT NULL,
    question TEXT NOT NULL,
    context_before TEXT,
    uncertain_text TEXT NOT NULL,
    context_after TEXT,
    location TEXT,
    options TEXT,  -- JSON
    allow_other INTEGER DEFAULT 1,
    allow_unknown INTEGER DEFAULT 1,
    confidence_without_answer REAL,
    is_critical INTEGER DEFAULT 0,
    status TEXT DEFAULT 'pending',
    answer TEXT,
    answered_at DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_clarifications_status ON clarifications(status);
CREATE INDEX idx_clarifications_document ON clarifications(document_id);

-- Corrections log
CREATE TABLE corrections (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id TEXT REFERENCES documents(id) ON DELETE CASCADE,
    original_text TEXT NOT NULL,
    corrected_text TEXT NOT NULL,
    context TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Processing queue
CREATE TABLE processing_queue (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT NOT NULL UNIQUE,
    status TEXT DEFAULT 'pending',
    priority INTEGER DEFAULT 0,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    started_at DATETIME,
    completed_at DATETIME
);

CREATE INDEX idx_queue_status ON processing_queue(status);

-- API usage tracking
CREATE TABLE api_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    provider TEXT NOT NULL,
    operation TEXT NOT NULL,
    document_id TEXT,
    tokens_used INTEGER,
    cost_usd REAL,
    response_time_ms INTEGER,
    success INTEGER NOT NULL,
    error_message TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_api_usage_provider ON api_usage(provider);
CREATE INDEX idx_api_usage_created ON api_usage(created_at);

-- Settings
CREATE TABLE settings (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### Implementation

```python
import aiosqlite
import json
from pathlib import Path
from datetime import datetime, date
from contextlib import asynccontextmanager

class Database:
    """
    SQLite database manager with FTS5 search.
    """

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._connection: aiosqlite.Connection | None = None

    async def connect(self) -> None:
        """Initialize database connection."""
        self._connection = await aiosqlite.connect(self.db_path)
        self._connection.row_factory = aiosqlite.Row

        # Enable WAL mode and foreign keys
        await self._connection.execute("PRAGMA journal_mode=WAL")
        await self._connection.execute("PRAGMA foreign_keys=ON")

    async def close(self) -> None:
        """Close database connection."""
        if self._connection:
            await self._connection.close()

    @asynccontextmanager
    async def transaction(self):
        """Context manager for transactions."""
        async with self._connection.execute("BEGIN"):
            try:
                yield
                await self._connection.commit()
            except Exception:
                await self._connection.rollback()
                raise

    async def initialize_schema(self) -> None:
        """Create tables if they don't exist."""
        schema_path = Path(__file__).parent / "schema.sql"
        schema = schema_path.read_text()

        await self._connection.executescript(schema)
        await self._connection.commit()

    # Document operations

    async def insert_document(self, doc: Document) -> None:
        """Insert a new document."""
        await self._connection.execute(
            """
            INSERT INTO documents (
                id, original_filename, archive_path, processed_at,
                document_type, date_value, date_confidence, date_source,
                summary, word_count, quality_score, language,
                provider_used, processing_time_ms,
                file_size_original, file_size_normalized
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                doc.id,
                doc.original_filename,
                str(doc.archive_path),
                doc.processed_at.isoformat(),
                doc.document_type,
                doc.date_value.isoformat() if doc.date_value else None,
                doc.date_confidence,
                doc.date_source,
                doc.summary,
                doc.word_count,
                doc.quality_score,
                doc.language,
                doc.provider_used,
                doc.processing_time_ms,
                doc.file_size_original,
                doc.file_size_normalized,
            ),
        )
        await self._connection.commit()

    async def get_document(self, doc_id: str) -> Document | None:
        """Get document by ID."""
        async with self._connection.execute(
            "SELECT * FROM documents WHERE id = ?",
            (doc_id,),
        ) as cursor:
            row = await cursor.fetchone()
            if row:
                return self._row_to_document(row)
            return None

    async def update_document(self, doc: Document) -> None:
        """Update existing document."""
        await self._connection.execute(
            """
            UPDATE documents SET
                summary = ?, word_count = ?, quality_score = ?,
                updated_at = ?
            WHERE id = ?
            """,
            (doc.summary, doc.word_count, doc.quality_score, datetime.now(), doc.id),
        )
        await self._connection.commit()

    # Full-text search

    async def update_fts(
        self,
        doc_id: str,
        transcription: str,
        entities_text: str,
    ) -> None:
        """Update FTS index for a document."""
        # Get current summary
        async with self._connection.execute(
            "SELECT summary FROM documents WHERE id = ?",
            (doc_id,),
        ) as cursor:
            row = await cursor.fetchone()
            summary = row["summary"] if row else ""

        # Update FTS
        await self._connection.execute(
            """
            INSERT OR REPLACE INTO documents_fts(id, transcription, summary, entities_text)
            VALUES (?, ?, ?, ?)
            """,
            (doc_id, transcription, summary, entities_text),
        )
        await self._connection.commit()

    async def search(
        self,
        query: str,
        limit: int = 50,
        offset: int = 0,
        filters: dict | None = None,
    ) -> list[SearchResult]:
        """
        Full-text search across documents.

        Args:
            query: Search query (FTS5 syntax supported)
            limit: Maximum results
            offset: Results offset
            filters: Optional filters (date_from, date_to, type, category)

        Returns:
            List of SearchResult with document and relevance
        """
        # Build query with optional filters
        where_clauses = []
        params = []

        if filters:
            if filters.get("date_from"):
                where_clauses.append("d.date_value >= ?")
                params.append(filters["date_from"])
            if filters.get("date_to"):
                where_clauses.append("d.date_value <= ?")
                params.append(filters["date_to"])
            if filters.get("type"):
                where_clauses.append("d.document_type = ?")
                params.append(filters["type"])

        where_sql = ""
        if where_clauses:
            where_sql = "AND " + " AND ".join(where_clauses)

        sql = f"""
            SELECT
                d.*,
                fts.rank AS relevance,
                snippet(documents_fts, 1, '<mark>', '</mark>', '...', 32) AS snippet
            FROM documents_fts fts
            JOIN documents d ON fts.id = d.id
            WHERE documents_fts MATCH ?
            {where_sql}
            ORDER BY fts.rank
            LIMIT ? OFFSET ?
        """

        params = [query] + params + [limit, offset]

        results = []
        async with self._connection.execute(sql, params) as cursor:
            async for row in cursor:
                results.append(SearchResult(
                    document=self._row_to_document(row),
                    relevance=row["relevance"],
                    snippet=row["snippet"],
                ))

        return results

    async def search_count(self, query: str, filters: dict | None = None) -> int:
        """Get total count for search query."""
        where_clauses = []
        params = [query]

        if filters:
            if filters.get("date_from"):
                where_clauses.append("d.date_value >= ?")
                params.append(filters["date_from"])
            if filters.get("date_to"):
                where_clauses.append("d.date_value <= ?")
                params.append(filters["date_to"])

        where_sql = ""
        if where_clauses:
            where_sql = "AND " + " AND ".join(where_clauses)

        sql = f"""
            SELECT COUNT(*) as count
            FROM documents_fts fts
            JOIN documents d ON fts.id = d.id
            WHERE documents_fts MATCH ?
            {where_sql}
        """

        async with self._connection.execute(sql, params) as cursor:
            row = await cursor.fetchone()
            return row["count"]

    # API usage tracking

    async def log_api_usage(
        self,
        provider: str,
        operation: str,
        document_id: str | None,
        response_time_ms: int,
        success: bool,
        error_message: str | None = None,
        tokens_used: int | None = None,
        cost_usd: float | None = None,
    ) -> None:
        """Log API usage for cost tracking."""
        await self._connection.execute(
            """
            INSERT INTO api_usage (
                provider, operation, document_id, tokens_used,
                cost_usd, response_time_ms, success, error_message
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                provider,
                operation,
                document_id,
                tokens_used,
                cost_usd,
                response_time_ms,
                1 if success else 0,
                error_message,
            ),
        )
        await self._connection.commit()

    async def get_api_usage_summary(
        self,
        provider: str | None = None,
        since: datetime | None = None,
    ) -> dict:
        """Get API usage summary."""
        where_clauses = []
        params = []

        if provider:
            where_clauses.append("provider = ?")
            params.append(provider)
        if since:
            where_clauses.append("created_at >= ?")
            params.append(since.isoformat())

        where_sql = ""
        if where_clauses:
            where_sql = "WHERE " + " AND ".join(where_clauses)

        sql = f"""
            SELECT
                provider,
                COUNT(*) as total_calls,
                SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as successful_calls,
                SUM(tokens_used) as total_tokens,
                SUM(cost_usd) as total_cost,
                AVG(response_time_ms) as avg_response_time
            FROM api_usage
            {where_sql}
            GROUP BY provider
        """

        results = {}
        async with self._connection.execute(sql, params) as cursor:
            async for row in cursor:
                results[row["provider"]] = {
                    "total_calls": row["total_calls"],
                    "successful_calls": row["successful_calls"],
                    "total_tokens": row["total_tokens"],
                    "total_cost": row["total_cost"],
                    "avg_response_time_ms": row["avg_response_time"],
                }

        return results

    # Statistics

    async def get_statistics(self) -> dict:
        """Get overall statistics."""
        stats = {}

        # Document counts
        async with self._connection.execute(
            "SELECT COUNT(*) as count FROM documents"
        ) as cursor:
            row = await cursor.fetchone()
            stats["total_documents"] = row["count"]

        # By type
        async with self._connection.execute(
            """
            SELECT document_type, COUNT(*) as count
            FROM documents
            GROUP BY document_type
            """
        ) as cursor:
            stats["by_type"] = {row["document_type"]: row["count"] async for row in cursor}

        # By month
        async with self._connection.execute(
            """
            SELECT strftime('%Y-%m', date_value) as month, COUNT(*) as count
            FROM documents
            WHERE date_value IS NOT NULL
            GROUP BY month
            ORDER BY month DESC
            LIMIT 12
            """
        ) as cursor:
            stats["by_month"] = {row["month"]: row["count"] async for row in cursor}

        # Pending clarifications
        async with self._connection.execute(
            "SELECT COUNT(*) as count FROM clarifications WHERE status = 'pending'"
        ) as cursor:
            row = await cursor.fetchone()
            stats["pending_clarifications"] = row["count"]

        return stats

    def _row_to_document(self, row: aiosqlite.Row) -> Document:
        """Convert database row to Document object."""
        return Document(
            id=row["id"],
            original_filename=row["original_filename"],
            archive_path=Path(row["archive_path"]),
            processed_at=datetime.fromisoformat(row["processed_at"]),
            document_type=row["document_type"],
            date_value=date.fromisoformat(row["date_value"]) if row["date_value"] else None,
            date_confidence=row["date_confidence"],
            date_source=row["date_source"],
            summary=row["summary"],
            word_count=row["word_count"],
            quality_score=row["quality_score"],
            language=row["language"],
            provider_used=row["provider_used"],
            processing_time_ms=row["processing_time_ms"],
            file_size_original=row["file_size_original"],
            file_size_normalized=row["file_size_normalized"],
        )
```

## Testing Strategy

### Unit Tests

```python
@pytest.mark.asyncio
async def test_insert_and_get_document(db):
    doc = Document(id="test123", original_filename="test.png", ...)

    await db.insert_document(doc)
    retrieved = await db.get_document("test123")

    assert retrieved is not None
    assert retrieved.id == "test123"

@pytest.mark.asyncio
async def test_fts_search(db):
    # Insert document with transcription
    doc = Document(id="doc1", ...)
    await db.insert_document(doc)
    await db.update_fts("doc1", "solar powered water heater invention", "")

    results = await db.search("solar heater")

    assert len(results) >= 1
    assert results[0].document.id == "doc1"

@pytest.mark.asyncio
async def test_api_usage_tracking(db):
    await db.log_api_usage(
        provider="azure",
        operation="transcribe",
        document_id="doc1",
        response_time_ms=1500,
        success=True,
    )

    summary = await db.get_api_usage_summary(provider="azure")

    assert summary["azure"]["total_calls"] == 1
    assert summary["azure"]["successful_calls"] == 1
```

## Acceptance Criteria

- [ ] Document metadata stored correctly
- [ ] Full-text search works with FTS5
- [ ] Search response < 500ms
- [ ] Category relationships work
- [ ] Entity storage works
- [ ] Clarification queue persists
- [ ] API usage tracked
- [ ] WAL mode enabled
- [ ] Concurrent access works
- [ ] Statistics queries work
