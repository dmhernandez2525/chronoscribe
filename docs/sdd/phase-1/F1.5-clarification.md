# F1.5 - Clarification System

## Overview

| Attribute | Value |
|-----------|-------|
| Feature ID | F1.5 |
| Phase | 1 (MVP) |
| Priority | P0 |
| Status | Planned |
| Module | `chronoscribe.services.clarification` |

## Description

Human-in-the-loop review system for uncertain transcriptions. The AI identifies low-confidence sections, generates specific contextual questions, and queues them for human review. Corrections are incorporated back into the transcription.

**This is ChronoScribe's primary differentiator** - no competitor offers this properly.

## Requirements

### Functional Requirements

1. **FR1.5.1** - Detect low-confidence sections (threshold: 0.7)
2. **FR1.5.2** - Generate specific, contextual questions
3. **FR1.5.3** - Limit to 5 questions per document (focus on most important)
4. **FR1.5.4** - Maintain persistent question queue
5. **FR1.5.5** - CLI interface for reviewing and answering
6. **FR1.5.6** - Update transcription with corrections
7. **FR1.5.7** - Auto-accept after configurable timeout (default: 7 days)
8. **FR1.5.8** - Track correction history for learning

### Non-Functional Requirements

1. **NFR1.5.1** - Question generation: < 5 seconds
2. **NFR1.5.2** - Questions are clear and specific
3. **NFR1.5.3** - No sensitive information in questions

## Technical Design

### Dependencies

```python
anthropic>=0.8.0    # For question generation
rich>=13.0.0        # CLI interface
```

### Module Structure

```
src/chronoscribe/services/
├── clarification.py        # Main clarification service
└── prompts/
    └── clarification.py    # Question generation prompts
```

### Type Definitions

```python
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Literal

class ClarificationStatus(Enum):
    PENDING = "pending"
    ANSWERED = "answered"
    AUTO_ACCEPTED = "auto_accepted"
    SKIPPED = "skipped"

class ClarificationType(Enum):
    WORD = "word"           # Single unclear word
    SECTION = "section"     # Unclear section/phrase
    MISSING = "missing"     # Completely illegible

@dataclass
class ClarificationOption:
    """A possible answer for a clarification question."""
    label: str
    description: str | None = None

@dataclass
class Clarification:
    """A clarification question for human review."""
    id: str
    document_id: str
    type: ClarificationType
    question: str
    context_before: str
    uncertain_text: str
    context_after: str
    location: str  # Human-readable location
    options: list[ClarificationOption]
    allow_other: bool = True
    allow_unknown: bool = True
    confidence_without_answer: float = 0.5
    is_critical: bool = False
    status: ClarificationStatus = ClarificationStatus.PENDING
    answer: str | None = None
    answered_at: datetime | None = None
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class ClarificationConfig:
    """Configuration for clarification system."""
    confidence_threshold: float = 0.7
    max_questions_per_document: int = 5
    auto_accept_days: int = 7
    critical_threshold: float = 0.5  # Below this, mark as critical
```

### Clarification Service

```python
import uuid
from anthropic import Anthropic
import json

class ClarificationService:
    """
    Manages human-in-the-loop clarification for uncertain transcriptions.

    Detects low-confidence sections, generates questions, and
    incorporates human corrections.
    """

    def __init__(
        self,
        anthropic_client: Anthropic,
        db: Database,
        config: ClarificationConfig | None = None,
    ):
        self.anthropic = anthropic_client
        self.db = db
        self.config = config or ClarificationConfig()

    async def analyze_transcription(
        self,
        document_id: str,
        transcription: TranscriptionResult,
        image_path: Path | None = None,
    ) -> list[Clarification]:
        """
        Analyze transcription and generate clarification questions.

        Args:
            document_id: The document ID
            transcription: The transcription result with confidence scores
            image_path: Optional path to image for visual context

        Returns:
            List of clarification questions (max 5)
        """
        # Find uncertain sections
        uncertain_blocks = self._find_uncertain_blocks(transcription)

        if not uncertain_blocks:
            return []

        # Generate questions using AI
        clarifications = await self._generate_questions(
            document_id,
            transcription.text,
            uncertain_blocks,
        )

        # Prioritize and limit
        clarifications = self._prioritize(clarifications)
        clarifications = clarifications[: self.config.max_questions_per_document]

        # Save to database
        for clarification in clarifications:
            await self.db.save_clarification(clarification)

        return clarifications

    def _find_uncertain_blocks(
        self,
        transcription: TranscriptionResult,
    ) -> list[TextBlock]:
        """Find text blocks below confidence threshold."""
        return [
            block
            for block in transcription.blocks
            if block.confidence < self.config.confidence_threshold
        ]

    async def _generate_questions(
        self,
        document_id: str,
        full_text: str,
        uncertain_blocks: list[TextBlock],
    ) -> list[Clarification]:
        """Generate clarification questions using AI."""
        prompt = self._build_prompt(full_text, uncertain_blocks)

        message = self.anthropic.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[{"role": "user", "content": prompt}],
        )

        response = json.loads(message.content[0].text)
        clarifications = []

        for item in response.get("clarifications", []):
            clarification = Clarification(
                id=f"clr_{uuid.uuid4().hex[:12]}",
                document_id=document_id,
                type=ClarificationType(item.get("type", "word")),
                question=item["question"],
                context_before=item.get("context_before", ""),
                uncertain_text=item.get("uncertain_text", ""),
                context_after=item.get("context_after", ""),
                location=item.get("location", "Unknown"),
                options=[
                    ClarificationOption(
                        label=opt["label"],
                        description=opt.get("description"),
                    )
                    for opt in item.get("options", [])
                ],
                allow_other=item.get("allow_other", True),
                allow_unknown=item.get("allow_unknown", True),
                confidence_without_answer=item.get("confidence_without_answer", 0.5),
                is_critical=item["id"] in response.get("critical_clarifications", []),
            )
            clarifications.append(clarification)

        return clarifications

    def _prioritize(self, clarifications: list[Clarification]) -> list[Clarification]:
        """Sort clarifications by priority (critical first, then by confidence)."""
        return sorted(
            clarifications,
            key=lambda c: (not c.is_critical, c.confidence_without_answer),
        )

    async def submit_answer(
        self,
        clarification_id: str,
        answer: str,
    ) -> None:
        """
        Submit a human answer to a clarification question.

        Args:
            clarification_id: The clarification ID
            answer: The human-provided answer
        """
        clarification = await self.db.get_clarification(clarification_id)
        if not clarification:
            raise ClarificationNotFoundError(clarification_id)

        clarification.answer = answer
        clarification.status = ClarificationStatus.ANSWERED
        clarification.answered_at = datetime.now()

        await self.db.update_clarification(clarification)

        # Update the transcription
        await self._update_transcription(clarification)

    async def _update_transcription(self, clarification: Clarification) -> None:
        """Update the document transcription with the correction."""
        document = await self.db.get_document(clarification.document_id)
        if not document:
            return

        # Load transcription file
        transcription_path = document.archive_path / "transcription.txt"
        if not transcription_path.exists():
            return

        text = transcription_path.read_text()

        # Replace uncertain text with answer
        if clarification.uncertain_text in text:
            text = text.replace(
                clarification.uncertain_text,
                clarification.answer,
            )
            transcription_path.write_text(text)

            # Also update markdown version
            md_path = document.archive_path / "transcription.md"
            if md_path.exists():
                md_text = md_path.read_text()
                md_text = md_text.replace(
                    clarification.uncertain_text,
                    clarification.answer,
                )
                md_path.write_text(md_text)

        # Log correction for learning
        await self.db.log_correction(
            document_id=clarification.document_id,
            original_text=clarification.uncertain_text,
            corrected_text=clarification.answer,
            context=clarification.context_before + clarification.context_after,
        )

    async def process_expired(self) -> int:
        """
        Auto-accept expired clarifications.

        Returns:
            Number of clarifications auto-accepted
        """
        cutoff = datetime.now() - timedelta(days=self.config.auto_accept_days)
        expired = await self.db.get_expired_clarifications(cutoff)

        count = 0
        for clarification in expired:
            clarification.status = ClarificationStatus.AUTO_ACCEPTED
            clarification.answered_at = datetime.now()
            await self.db.update_clarification(clarification)
            count += 1

        return count

    async def get_pending_count(self) -> int:
        """Get count of pending clarifications."""
        return await self.db.count_pending_clarifications()

    async def get_pending(
        self,
        limit: int = 10,
        document_id: str | None = None,
    ) -> list[Clarification]:
        """Get pending clarifications."""
        return await self.db.get_pending_clarifications(
            limit=limit,
            document_id=document_id,
        )
```

### CLI Interface

```python
import click
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt, Confirm

console = Console()

@click.command()
@click.option("--document", "-d", help="Filter by document ID")
@click.option("--critical", "-c", is_flag=True, help="Show only critical")
def clarify(document: str | None, critical: bool):
    """Review and answer clarification questions."""
    service = get_clarification_service()

    pending = asyncio.run(service.get_pending(document_id=document))

    if critical:
        pending = [c for c in pending if c.is_critical]

    if not pending:
        console.print("[green]No pending clarifications![/green]")
        return

    console.print(f"\n[bold]Found {len(pending)} pending clarifications[/bold]\n")

    for i, clarification in enumerate(pending, 1):
        _display_clarification(clarification, i, len(pending))

        # Show options
        options = {}
        for j, opt in enumerate(clarification.options, 1):
            options[str(j)] = opt.label
            desc = f" - {opt.description}" if opt.description else ""
            console.print(f"  [{j}] {opt.label}{desc}")

        if clarification.allow_other:
            options["o"] = "other"
            console.print("  [o] Other (type your answer)")

        if clarification.allow_unknown:
            options["u"] = "unknown"
            console.print("  [u] Mark as unknown")

        console.print("  [s] Skip")
        console.print()

        # Get answer
        choice = Prompt.ask("Your choice", choices=list(options.keys()) + ["s"])

        if choice == "s":
            continue
        elif choice == "o":
            answer = Prompt.ask("Enter your answer")
        elif choice == "u":
            answer = "[unknown]"
        else:
            answer = options[choice]

        # Submit
        asyncio.run(service.submit_answer(clarification.id, answer))
        console.print("[green]Answer recorded![/green]\n")


def _display_clarification(c: Clarification, index: int, total: int):
    """Display a clarification question."""
    status = "[red]CRITICAL[/red] " if c.is_critical else ""

    panel_content = f"""
{status}[bold]{c.question}[/bold]

[dim]Context:[/dim]
...{c.context_before}[yellow]{c.uncertain_text}[/yellow]{c.context_after}...

[dim]Location:[/dim] {c.location}
"""

    console.print(Panel(
        panel_content.strip(),
        title=f"Question {index}/{total}",
        border_style="yellow" if c.is_critical else "blue",
    ))
```

## Database Schema

```sql
CREATE TABLE clarifications (
    id TEXT PRIMARY KEY,
    document_id TEXT REFERENCES documents(id),
    type TEXT NOT NULL,
    question TEXT NOT NULL,
    context_before TEXT,
    uncertain_text TEXT NOT NULL,
    context_after TEXT,
    location TEXT,
    options TEXT,  -- JSON array
    allow_other INTEGER DEFAULT 1,
    allow_unknown INTEGER DEFAULT 1,
    confidence_without_answer REAL,
    is_critical INTEGER DEFAULT 0,
    status TEXT DEFAULT 'pending',
    answer TEXT,
    answered_at DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_clarifications_status ON clarifications(status);
CREATE INDEX idx_clarifications_document ON clarifications(document_id);
CREATE INDEX idx_clarifications_created ON clarifications(created_at);

-- Corrections log for learning
CREATE TABLE corrections (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id TEXT REFERENCES documents(id),
    original_text TEXT NOT NULL,
    corrected_text TEXT NOT NULL,
    context TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

## Testing Strategy

### Unit Tests

```python
def test_find_uncertain_blocks():
    service = ClarificationService(mock_anthropic, mock_db)
    transcription = TranscriptionResult(
        text="Hello world",
        blocks=[
            TextBlock(text="Hello", confidence=0.95),
            TextBlock(text="world", confidence=0.6),  # Below threshold
        ],
        provider=TranscriptionProvider.AZURE,
        confidence=0.77,
        word_count=2,
        processing_time_ms=100,
    )

    uncertain = service._find_uncertain_blocks(transcription)

    assert len(uncertain) == 1
    assert uncertain[0].text == "world"

def test_prioritize_puts_critical_first():
    service = ClarificationService(mock_anthropic, mock_db)
    clarifications = [
        Clarification(id="1", is_critical=False, confidence_without_answer=0.3, ...),
        Clarification(id="2", is_critical=True, confidence_without_answer=0.6, ...),
        Clarification(id="3", is_critical=False, confidence_without_answer=0.5, ...),
    ]

    result = service._prioritize(clarifications)

    assert result[0].id == "2"  # Critical first
```

## Acceptance Criteria

- [ ] Low-confidence sections detected (threshold: 0.7)
- [ ] Questions are specific and contextual
- [ ] Maximum 5 questions per document
- [ ] Queue persists across sessions
- [ ] CLI interface works for review
- [ ] Corrections update transcription files
- [ ] Auto-accept after 7 days (configurable)
- [ ] Corrections logged for future learning
- [ ] Critical clarifications highlighted
